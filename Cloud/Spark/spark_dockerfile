FROM ubuntu:latest
# Now install the needed packages.
RUN apt-get -y update && apt-get install -y default-jdk python3
RUN apt-get install -y python3-dev python3-pip vim git less
RUN pip3 install pip
RUN pip3 install pyspark
RUN pip3 install requests
RUN pip3 install numpy
RUN pip3 install cloudant
# Note, I have added several other packages that provide networking utilities like
# ping, nslookup, ifconfig etc.
RUN apt-get -y update && apt-get install -y net-tools wget dnsutils iputils-ping iputils-tracepath iputils-arping iputils-clockdiff
# Here we are hardcoding the download mirror and the spark version. I am sure
# there will be another and better way to do this
RUN wget https://downloads.apache.org/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz
RUN zcat spark-3.2.0-bin-hadoop3.2.tgz | tar xpof -
COPY spark-env.sh /spark-3.2.0-bin-hadoop3.2/conf/
COPY spark-worker.conf /spark-3.2.0-bin-hadoop3.2/conf/
COPY spark-driver.conf /spark-3.2.0-bin-hadoop3.2/conf/
COPY MapReduce.py /
# Now we set environment variable that we will need in the container at runtime
ENV SPARK_HOME=/spark-3.2.0-bin-hadoop3.2
ENV PATH=${PATH}:${SPARK_HOME}/sbin:${SPARK_HOME}/bin