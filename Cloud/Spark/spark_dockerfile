    FROM ubuntu:latest
    # Now install the needed packages.
    RUN apt-get -y update && apt-get install -y default-jdk python3
    RUN apt-get install -y python3-full python3-pip pipx vim git less python3-venv
   # RUN python3 -m venv .venv
    #RUN . .venv/bin/activate
    RUN pipx install pyspark
    RUN pipx install requests --include-deps
    RUN pipx install numpy
    RUN pipx install cloudant --include-deps
# Note, I have added several other packages that provide networking utilities like
# ping, nslookup, ifconfig etc.
RUN apt-get -y update && apt-get install -y net-tools wget dnsutils iputils-ping iputils-tracepath iputils-arping iputils-clockdiff
# Here we are hardcoding the download mirror and the spark version. I am sure
# there will be another and better way to do this
RUN wget https://dlcdn.apache.org/spark/spark-3.5.3/spark-3.5.3-bin-hadoop3.tgz
RUN zcat spark-3.5.3-bin-hadoop3.tgz | tar xpof -
COPY spark-env.sh /spark-3.5.3-bin-hadoop3/conf/
COPY spark-worker.conf /spark-3.5.3-bin-hadoop3/conf/
COPY spark-driver.conf /spark-3.5.3-bin-hadoop3/conf/
COPY MapReduce.py /
# Now we set environment variable that we will need in the container at runtime
ENV SPARK_HOME=/spark-3.5.3-bin-hadoop3
ENV PATH=${PATH}:${SPARK_HOME}/sbin:${SPARK_HOME}/bin